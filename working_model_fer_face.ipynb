{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XX6JUpmwGyN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "outputId": "1cdaeb99-c660-4775-b75f-7a27beec74f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'shape'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-3a11cc342ade>\u001b[0m in \u001b[0;36m<cell line: 80>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdivideFramesAndCutHalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/A1B1.mp4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-3a11cc342ade>\u001b[0m in \u001b[0;36mdivideFramesAndCutHalf\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     46\u001b[0m   \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m   \u001b[0;31m# Get frame dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m   \u001b[0mframe_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
          ]
        }
      ],
      "source": [
        "!pip install fer\n",
        "import matplotlib.pyplot as plt\n",
        "from fer import FER\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def predictUsingFer(img):\n",
        "  detector = FER()\n",
        "  arr = detector.detect_emotions(img)\n",
        "  print(arr)\n",
        "  max=0.0\n",
        "  if arr:  # Check if any faces were detected\n",
        "    emotions = arr[0]['emotions']\n",
        "    if emotions['angry']>max:\n",
        "        max = emotions['angry']\n",
        "    if emotions['disgust']>max:\n",
        "        max = emotions['disgust']\n",
        "    if emotions['fear']>max:\n",
        "        max = emotions['fear']\n",
        "    if emotions['happy']>max:\n",
        "        max = emotions['happy']\n",
        "    if emotions['sad']>max:\n",
        "        max = emotions['sad']\n",
        "    if emotions['surprise']>max:\n",
        "        max = emotions['surprise']\n",
        "  else:\n",
        "    print(\"No faces detected in the image.\")\n",
        "    plt.imshow(img)\n",
        "    plt.show()\n",
        "    return 0\n",
        "  return max\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Initialize video capture\n",
        "def divideFramesAndCutHalf(path):\n",
        "  vidcap = cv2.VideoCapture(path)\n",
        "  success, image = vidcap.read()\n",
        "  fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
        "  count = 0\n",
        "  sum=0.0\n",
        "  numOfDetetedFaces=0\n",
        "  percenteveryXsec=[]\n",
        "  X=10\n",
        "  # Get frame dimensions\n",
        "  frame_height, frame_width = image.shape[:2]\n",
        "\n",
        "  while success:\n",
        "      # Crop the frame to the left half\n",
        "      left_half = image[:, :frame_width // 2]\n",
        "\n",
        "      # Convert the cropped frame to RGB\n",
        "      left_half_rgb = cv2.cvtColor(left_half, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "      acc=predictUsingFer(left_half_rgb)\n",
        "      if(acc!=0):\n",
        "        numOfDetetedFaces=numOfDetetedFaces+1\n",
        "      sum=sum+acc\n",
        "      if (count-151)!=0 and (count-151)%(X*int(fps))==0:\n",
        "        if (numOfDetetedFaces!=0):\n",
        "          percenteveryXsec.append(sum/numOfDetetedFaces)\n",
        "        else:\n",
        "          percenteveryXsec.append(0)\n",
        "        numOfDetetedFaces=0\n",
        "        sum=0\n",
        "      success, image = vidcap.read()\n",
        "      count += 1\n",
        "      print('Read a new frame: ', success)\n",
        "\n",
        "  # Release the video capture object\n",
        "  vidcap.release()\n",
        "\n",
        "  return percenteveryXsec\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(divideFramesAndCutHalf(\"/content/A1B1.mp4\"))"
      ]
    }
  ]
}